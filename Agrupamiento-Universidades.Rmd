---
title: "Agrupamiento-Universidades"
author: "Brayan Ortiz, Juan Peña, Thalea Hesse, Juan Falcon, Daniel Espinal"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(convertr)
library(dplyr)
library(stringr)
library(factoextra)
library(zoo)
library(purrr)
install.packages("Gifi", repos="http://R-Forge.R-project.org")
library(Gifi)
```


# Lectura de datos

```{r}
datos <- read.csv("datos/CollegeScorecard.csv")
```

# Procesamiento de los datos

El conjunto de datos cuentas con 1725 características, sin embargo muchas de ellas no contienen información. Por esta razón se deciden eliminar las característica que tengan más del 20% en valores NaN. Por otra parte, el conjunto de datos contiene diferentes formas para representar los valores vacíos, por ejemplo cadenas de texto vacías o con espacios y, adicionalmente, hay registros con valores en *PrivacySuppressed*. Todos estos valores se recopilaron y se replazaron por el objeto NA de R.  

Existen variables (INSTNM, NPCURL, INSTURL) con todos los valores, pero no aportan información para el análisis. Estas columnas también fueron eliminadas del conjunto de datos.


```{r}
# Reemplazar algunos valores con NA
df <- replace(datos, datos=="PrivacySuppressed", NA) # revisar: parece que quedan como strings
df <- replace(df, df==" ", NA)
df <- replace(df, df=="", NA)

# Se borran columnas con valores nulos (>20%)
df <- df[,-which(colMeans(is.na(df)) >= 0.2)]

# Se borran filas con valores nulos
# delete.na <- function(df, n=0) {
#  df[rowSums(is.na(df)) <= n,]
# }
# df <- delete.na(df)

# Eliminar columnas manualmente
df <- subset(df, select = -c(INSTNM, NPCURL, INSTURL, UNITID, 
                             OPEID, opeid6))

# Algunas variables, a pesar de ser numéricas, tienen muy poca variabilidad: poolyrs
```

## Convertir los tipos de datos

```{r}
columnas <- data.frame(colnames(df))

columnas_CIP <- columnas %>% 
  filter(str_detect(colnames.df., "^CIP"))

df <- df %>% 
  mutate_at(vars(main, region, CONTROL, CURROPER, HCM2, LOCALE, DISTANCEONLY, STABBR, st_fips,
                 HBCU, PBI, ANNHI, TRIBAL, AANAPII, HSI, NANTI, MENONLY, WOMENONLY, PREDDEG, HIGHDEG,
                 CURROPER), 
            list(as.factor)) %>% 
  mutate_at(vars(columnas_CIP$colnames.df.), list(as.factor))

# st_fips se puede borrar y dejar solo region
```

```{r}
#Separar df por tipo de columna
# colSums(is.na(df)/7804)

#Convertir todos los chr a factor
df <- df %>% mutate(across(.cols=where(is.character), .fns=as.factor))

df_fact <- df[sapply(df, is.factor)] # Acá quedan variables numéricas (RARO xd)

index_i <- grep("CURROPER", colnames(df_fact))+1
index_s <- ncol(df_fact)

df_aux <- df_fact[c(index_i:index_s)]
df_fact <- df_fact[c(0:(index_i-1))]

df_aux <- mutate_all(df_aux, as.numeric)


df_num <- df %>% 
    select_if(is.numeric)

df_num <- cbind(df_num, df_aux)

# addNA para añadir NA como level del factor.
df_fact <- modify(df_fact, addNA)
#rowSums(is.na(df_fact))
```
 
## Interpolación de los datos faltantes

```{r}
df_num <- rapply(df_num, zoo::na.fill,"numeric",fill="extend",how="replace")

df_num_scale <- scale(df_num, center = TRUE, scale = TRUE)
```

La reducción de componentes se hará con variables relacionadas.

```{r}
cols_nums <- data.frame(colnames(df_num_scale))

# Obtiene el rango de columnas llamadas PCIP*
df_pcip <- cols_nums %>% 
  filter(str_detect(colnames.df_num_scale., "^PCIP"))

df_pcip <- subset(df_num_scale, select = df_pcip$colnames.df_num_scale.)

# Obtiene el rango de columnas llamadas UGDS*
df_ugds <- cols_nums %>% 
  filter(str_detect(colnames.df_num_scale., "^UGDS"))

df_ugds <- subset(df_num_scale, select = df_ugds$colnames.df_num_scale.)
```

# Componentes principales

```{r}
nums_cp <- princomp(df_num_scale)

plot(nums_cp)

prop_expl_var<-cumsum((nums_cp$sdev)^2)/sum((nums_cp$sdev)^2)

npc_opt<-which.min(abs(prop_expl_var-0.8)) # Así se encuentra el número de componentes principales que explican aproximadamente el 80% de la variabilidad


# npc<-20
plot(prop_expl_var,type="h",las=1,xlim=c(1,38),ylab="Proporción de varianza explicada",xlab="m")
points(npc_opt,prop_expl_var[npc_opt],col="red",lwd=2)
segments(x0=0,y0=prop_expl_var[npc_opt],x1=npc_opt,y1=prop_expl_var[npc_opt],col="red",lwd=2)
segments(x0=npc_opt,y0=0,x1=npc_opt,y1=prop_expl_var[npc_opt],col="red",lwd=2)


# pcip_cp <- princomp(df_pcip)
# 
# plot(pcip_cp)
# 
# prop_expl_var<-cumsum((pcip_cp$sdev)^2)/sum((pcip_cp$sdev)^2)
# 
# npc_opt<-which.min(abs(prop_expl_var-0.8)) # Así se encuentra el número de componentes principales que explican aproximadamente el 80% de la variabilidad
# 
# 
# # npc<-20
# plot(prop_expl_var,type="h",las=1,xlim=c(1,38),ylab="Proporción de varianza explicada",xlab="m")
# points(npc_opt,prop_expl_var[npc_opt],col="red",lwd=2)
# segments(x0=0,y0=prop_expl_var[npc_opt],x1=npc_opt,y1=prop_expl_var[npc_opt],col="red",lwd=2)
# segments(x0=npc_opt,y0=0,x1=npc_opt,y1=prop_expl_var[npc_opt],col="red",lwd=2)


# ugds_cp <- prcomp(df_ugds, scale = TRUE, center = TRUE)
# summary(ugds_cp)
# 
# pcip_cp <- prcomp(df_pcip, scale = TRUE, center = TRUE)
# summary(pcip_cp)
# 
# all_cp <- prcomp(df_num, scale = FALSE, center = TRUE)
# summary(all_cp)
```

```{r}
# eig_ugds <- get_eigenvalue(ugds_cp)
# plot(eig_ugds$cumulative.variance.percent, eig_ugds$eigenvalue)
# 
# eig_pcip <- get_eigenvalue(pcip_cp)
# eig_pcip
# fviz_eig(pcip_cp, ncp = 30)
# fviz_pca_var(ugds_cp, col.var="steelblue")
# 
# pca_1_2 <- data.frame(all_cp$x[, 17:43])
# 
# fviz_pca_var(ugds_cp, col.var="steelblue")
# plot(all_cp$x[,17], all_cp$x[,43])
```

# Componentes principales de los datos categóricos

```{r}
# Ejemplo: http://www.css.cornell.edu/faculty/dgr2/_static/files/R_html/NonlinearPCA.html
# pero no termina ... 
# cols_prin_fact <- princals(df_fact, ndim = 2, ordinal = FALSE, itmax=5)

```


# Modelo

```{r}
```
