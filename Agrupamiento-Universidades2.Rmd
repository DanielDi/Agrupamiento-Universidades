---
title: "Agrupamiento-Universidades"
author: "Brayan Ortiz, Juan Peña, Thalea Hesse, Juan Sebastian Falcon, Daniel Espinal"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(convertr)
library(dplyr)
library(stringr)
library(factoextra)
library(zoo)
library(purrr)
library(Gifi)
library(klaR)
library(clustMixType)
library(reshape2)
library(ggplot2)
```


# Lectura de datos

```{r, echo=FALSE}
datos <- read.csv("datos/CollegeScorecard.csv")
```

# Procesamiento de los datos

El conjunto de datos cuentas con 1725 características, sin embargo muchas de ellas no contienen información. Por esta razón se deciden eliminar las característica que tengan más del 20% en valores NaN. Por otra parte, el conjunto de datos contiene diferentes formas para representar los valores vacíos, por ejemplo cadenas de texto vacías o con espacios y, adicionalmente, hay registros con valores en *PrivacySuppressed*. Todos estos valores se recopilaron y se replazaron por el objeto NA de R.  

Existen variables (INSTNM, NPCURL, INSTURL) con todos los valores, pero no aportan información para el análisis. Estas columnas también fueron eliminadas del conjunto de datos.


```{r, echo=FALSE}
# Reemplazar algunos valores con NA
df <- replace(datos, datos=="PrivacySuppressed", NA) # revisar: parece que quedan como strings
df <- replace(df, df==" ", NA)
df <- replace(df, df=="", NA)

# Se borran columnas con valores nulos (>20%)
df <- df[,-which(colMeans(is.na(df)) >= 0.2)]

# # Se borran filas con valores nulos
delete.na <- function(df, n=0) {
 df[rowSums(is.na(df)) <= n,]
}
df <- delete.na(df)

# Eliminar columnas manualmente
df <- subset(df, select = -c(INSTNM, NPCURL, INSTURL, UNITID, 
                             OPEID, opeid6))

# TODO EXPLICAR PORQUE SACAMOS LAS SGTES
df <- subset(df, select = -c(LATITUDE, LONGITUDE, ZIP, STABBR, st_fips, HBCU, 
                             PBI, ANNHI, TRIBAL, AANAPII, HSI, NANTI, MENONLY, 
                             WOMENONLY))


# Algunas variables, a pesar de ser numéricas, tienen muy poca variabilidad: poolyrs
```

## Convertir los tipos de datos

```{r, echo=FALSE}
columnas <- data.frame(colnames(df))

columnas_CIP <- columnas %>% 
  filter(str_detect(colnames.df., "^CIP"))

df <- df %>% 
  mutate_at(vars(main, region, CONTROL, CURROPER, HCM2, LOCALE, DISTANCEONLY,
                 PREDDEG, HIGHDEG, CURROPER), 
            list(as.factor)) %>% 
  mutate_at(vars(columnas_CIP$colnames.df.), list(as.factor))

# st_fips se puede borrar y dejar solo region
```

```{r, echo=FALSE}
#Separar df por tipo de columna
# colSums(is.na(df)/7804)

#Convertir todos los chr a factor
df <- df %>% mutate(across(.cols=where(is.character), .fns=as.factor))

df_fact <- df[sapply(df, is.factor)] # Acá quedan variables numéricas (RARO xd)

# Obtiene las numéricas que fueron leídas como categóricas
index_i <- grep("CURROPER", colnames(df_fact))+1
index_s <- ncol(df_fact)

df_aux <- df_fact[c(index_i:index_s)]
df_fact <- df_fact[c(0:(index_i-1))]

df_aux <- mutate_all(df_aux, as.numeric)

# Selecciona solo las numéricas
df_num <- df %>% 
    select_if(is.numeric)

# Une los dos datasets numéricos
df_num <- cbind(df_num, df_aux)

# addNA para añadir NA como level del factor.
df_fact <- modify(df_fact, addNA)

# Elimina las variables CIPÖ*
df_fact_nocip <- df_fact[,!(colnames(df_fact) %in% columnas_CIP$colnames.df.)]

#rowSums(is.na(df_fact))
```
 
## Interpolación de los datos faltantes
Los datos numéricos faltantes se completan realizando una extrapolación de los datos que se tienen en la base datos. Se hacen separado para los datos numéricos y enteros. 

```{r, echo=FALSE}
# # Interpolación para los datos numéricos
# df_num <- rapply(df_num, zoo::na.fill,"numeric",fill="extend",how="replace")
# 
# # Interpolacińo para los datos enteros
# df_num <- rapply(df_num, zoo::na.fill,"integer",fill="extend",how="replace")

# Reemplazar nan con la media
# for(i in 1:ncol(df_num)){
#   df_num[is.na(df_num[,i]), i] <- mean(df_num[,i], na.rm = TRUE)
# }

# Estandarización de los datos
# df_num_scale <- scale(df_num, center = TRUE, scale = TRUE)

maxs <- apply(df_num, 2, max)
mins <- apply(df_num, 2, min)
df_num_scale <- scale(df_num, center = mins, scale = maxs - mins)
```

# Componentes principales

Las variables PCIP tienen el siguiente resumen de las componentes:  

```{r, echo=FALSE}
main_pca <- princomp(df_num_scale)
summary(main_pca)
```
De lo anterior, se puede observar que las primeras 2 componentes explican, aproximadamente, el 80% de la variabilidad. En el siguiente gráfico se muestra la varianza explicada por la primeras 10 componentes: 

```{r, echo=FALSE}
plot(main_pca, main = "Varianza por cada componente")
```

Para determinar la cantidad óptima de componentes con base a una explicación de la variabilidad del 80%, se calcula la variabilidad acumulada de las componentes y se obtiene la mínima componente con un 80% aproximado de esta: 

```{r, echo=FALSE}
# Variabilidda acumulada
prop_expl_var <- cumsum((main_pca$sdev)^2)/sum((main_pca$sdev)^2)

# Número óptimo de componentes con, aproximadamente, un 80% de explicación de la variabilidad
npc_opt <- which.min(abs(prop_expl_var-0.8))
npc_opt
```

Para ver gráficamente lo anterior: 

```{r, echo=FALSE}
plot(prop_expl_var,
     type = "h", las = 1, xlim = c(1,90),
     ylab = "Proporción de varianza explicada", xlab = "m", 
     main = "Número óptimo de componentes principales")
points(npc_opt, prop_expl_var[npc_opt],
       col = "red", lwd = 2)

segments(x0 = 0 ,y0 = prop_expl_var[npc_opt],
         x1 = npc_opt, y1 = prop_expl_var[npc_opt],
         col = "red", lwd = 2)
segments(x0 = npc_opt, y0 = 0, x1 = npc_opt,
         y1 = prop_expl_var[npc_opt], col = "red", lwd = 2)
```

TODO EXPLICAR LAS COMPONENTES

## Reconstrucción del conjunto de datos

Como se calcularon las componentes para un subconjunto de variables, entonces es necesario unificar estos nuevos valores proyectados con las variables a las cuales no se les calculó componentes. Para esto, se eliminan del conjunto de datos las variables que fueron proyectadas y se añaden sus proyecciones, mientras que las demás variables permanencen iguales.
```{r}
eigen <- main_pca$loadings[, 1:npc_opt]

# Valores de PCIP proyectados en las componentes
df_proy <- df_num_scale%*%eigen
# colnames(df_proy) <- paste(colnames(df_proy), "_PCIP", sep="")
head(df_proy)
```


# Modelo

```{r, echo=FALSE}
# Clúster para todos incluido los caracteristicas
set.seed(220612)
df_final <- cbind(as.data.frame(df_proy), df_fact_nocip)

clusters_final <- kproto(df_final, 5, na.rm = FALSE, verbose=FALSE)
clusters_final$centers
clprofiles(clusters_final, df_final)
```
```{r}
df_final$clus <- as.factor(clusters_final$cluster)
plot(df_final$Comp.1, df_final$Comp., col=df_final$clus)
```

```{r}
# Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
#data <- na.omit(df_final) # to remove the rows with NA's
wss <- sapply(1:k.max, 
              function(k){kproto(df_final, k, na.rm = FALSE)$tot.withinss})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```


```{r, echo=FALSE}
# Cluśter de todo el conjunto de datos
clusters_final <- kproto(df_final, 4, na.rm = FALSE, verbose=FALSE)
clprofiles(clusters_final, df_final)
```
```{r}
kmeans(df_final, 4)
```

```{r}
#Importancia de cada variable por componente
melted <- melt(main_pca$loadings[,1:9])
ggplot(data = melted) +
  theme(legend.position = "none", axis.text.x = element_blank(), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Measurements in College Scorecard data",
       y = "Relative importance in each principle component",
       title = "Variables in Principal Component Analysis") +
  geom_bar(aes(x=Var1, y=value, fill=Var1), stat="identity") +
  facet_wrap(~Var2)

melted$value_abs<-abs(melted$value)
melted_sort <- melted[order(melted$value_abs,decreasing = T),]


#Individualmente para ver cada variable

#Componente1
melted_sort1<-melted_sort %>%  filter(Var2=="Comp.1") %>% head(10)

ggplot(data = melted_sort1[melted_sort1$Var2 == "Comp.1",]) +
  theme(legend.position = "none", 
        axis.text.x= element_text(angle=45, hjust = 1), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Variables ",
       y = "Relative importance in principle component",
       title = "Variables in PC1") +
  geom_bar(aes(x=reorder(Var1,-value_abs), y=value_abs, fill=Var1), stat="identity")


#Componente 2
melted_sort2<-melted_sort %>% filter(Var2=="Comp.2") %>% head(10)

ggplot(data = melted_sort2[melted_sort2$Var2 == "Comp.2",]) +
  theme(legend.position = "none", 
        axis.text.x= element_text(angle=45, hjust = 1), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Variables ",
       y = "Relative importance in principle component",
       title = "Variables in PC2") +
  geom_bar(aes(x=reorder(Var1,-value_abs), y=value_abs, fill=Var1), stat="identity")

#Componente 3
melted_sort3<-melted_sort %>% filter(Var2=="Comp.3") %>% head(10)

ggplot(data = melted_sort3[melted_sort3$Var2 == "Comp.3",]) +
  theme(legend.position = "none", 
        axis.text.x= element_text(angle=45, hjust = 1), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Variables ",
       y = "Relative importance in principle component",
       title = "Variables in PC3") +
  geom_bar(aes(x=reorder(Var1,-value_abs), y=value_abs, fill=Var1), stat="identity")


```




# Bibliografía

[1] G. James, *Et all*, *An Introduction to Statistical Learning with  Applications in R*. New York :Springer, 2013. [E-book].  
[2] Klaudia. Bury, 'Clustering on PCA results', 2021. [Online]. Available: https://rpubs.com/Bury/ClusteringOnPcaResults. [Accessed: 11- Jun- 2022]
[3] Luke. Hayden, 'Principal Component Analysis in R Tutorial ', 2018. [Online]. Available: https://www.datacamp.com/tutorial/pca-analysis-r. [Accessed: 09- Jun- 2022]
